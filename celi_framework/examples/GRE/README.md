# GRE Writing Task Assistant

## Problem Statement

The GRE (Graduate Record Examination) writing section, known as the Analytical Writing Assessment, comprises two tasks:

1. Analyze an Issue
2. Analyze an Argument

This example focuses on the "Analyze an Issue" task, using CELI to generate high-scoring essays.

## Our Solution

We've developed a system that leverages CELI to write essays that consistently score 6 on the GRE rubric. Our approach involves:

1. **Job Descriptions**: Detailed instructions for CELI on how to approach and write these essays.
2. **Custom Tools**: A set of tools to assist in the writing process, automatically invoked by CELI controllers as needed.

The overall use case is defined in the CELI Job Description object.  When you run CELI, you pass general configuration parameters and a Job Description to the main CELI processor.  The `JobDescription` defines the tasks to be accomplished and the tools to be run.  See (job_description.py)[celi-framework/core/job_description.py] for full details.

The job description contains several prompt strings which describe the overall job to be run at a high level along with any general guidance for the agent.  It also contains a `task_list` and a `tool_implementations_class`.

The `task_list` is a list of `Task` objects.  When completing a job, the agent will tackle each task in this list in order.  Each task has a name and a set of details.  The details is a dictionary that will be passed directly to the LLM to describe how to accomplish the task.  

The `tool_implementations_class` is a reference to a class that derives from `ToolImplementations` and contains the tools that the LLM can use to accomplish the task.  This class is described in the next section.

## Key Features

- Consistently generates essays scoring 6 on the GRE rubric
- Automated tool integration for enhanced writing support
- Scalable solution adaptable to various "Analyze an Issue" prompts

## Project Structure

```
GRE/
├── data/
│   └── scoring_guidelines/
├── tools.py
├── job_description.py
├── main.py
└── README.md
```

## Quick Start

To run CELI against the HumanEval benchmark:

1. Set up your environment:
   - Ensure your `.env` file is correctly configured (refer to `.env.example`).
   - Set `JOB_DESCRIPTION=celi_framework.examples.GRE.job_description.job_description` in your `.env` file.

2. Run the benchmark:
   - From the root of the project directory, execute:
     ```
     python -m celi_framework.main
     ```
   - This will generate a JSON output file (with timestamp) in `GRE/target/celi_output/drafts/`.

## Evaluation

Essays generated by this system are scored using the official GRE rubric, found in `GRE/data/scoring_guidelines/`

